================================================================================
OPTIMIZER & LEARNING RATE BENCHMARK REPORT
================================================================================

Objective:
  Compare Adam vs AdamW with different learning rates for Stage 3 training.
  Current baseline: Adam with LR=5e-6, producing Val Loss=1.32

Hypothesis:
  - LR too conservative (5e-6) â†’ slow convergence
  - AdamW better weight decay handling for 138M parameters
  - Higher LR (1e-5, 2e-5) could improve final performance

================================================================================
RESULTS SUMMARY
================================================================================

Rank   | Config       |   Val Loss |    Val Acc |  Converged
--------------------------------------------------------------------------------
1      | AdamW_2e-5   |     1.1233 |     60.47% |       True ***
2      | Adam_2e-5    |     1.1184 |     60.26% |       True 
3      | AdamW_1e-5   |     1.1933 |     57.82% |       True 
4      | Adam_1e-5    |     1.1972 |     56.34% |       True 
5      | AdamW_5e-6   |     1.2347 |     56.20% |       True 
6      | Adam_5e-6    |     1.2418 |     55.92% |       True 

================================================================================
BEST CONFIGURATION
================================================================================

Config: AdamW_2e-5
  Optimizer: ADAMW
  Learning Rate: 2e-05
  Weight Decay: 5e-05

Performance:
  Validation Loss: 1.1233
  Validation Accuracy: 60.47%
  Converged: Yes

Improvement over baseline (Adam_5e-6):
  Val Loss: 1.2418 -> 1.1233 (-0.1185)
  Val Acc: 55.92% -> 60.47% (+4.55%)

================================================================================
RECOMMENDATIONS
================================================================================

1. Use the best configuration for Stage 3 training:
   python scripts/train_stage3_deep.py \
     --lr 2e-05 \
     --weight-decay 5e-05

2. Switch to AdamW optimizer in training script:
   optimizer = optim.AdamW(model.parameters(), ...)

3. Expected improvements:
   - Better convergence (loss reduction)
   - Improved validation accuracy
   - More stable training

